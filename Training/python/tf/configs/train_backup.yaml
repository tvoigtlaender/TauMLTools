hydra: # disable usage of hydra log directory as cwd
  job:
    chdir: False

defaults:
  - model@model: transformer
  - _self_

# mlflow
path_to_mlflow: ./mlruns
experiment_name: ShuffleMergeSpectral_trainingSamples-2_rerun_2TF
# experiment_name: ???

# datasets
dataset_name: ShuffleMergeSpectral_trainingSamples-2_rerun_2TF # to log to mlflow for bookkeeping
# dataset_name: ??? # to log to mlflow for bookkeeping
datasets: 
  train:
    ShuffleMergeSpectral_trainingSamples-2_rerun_2TF: "../../../.." # will take all files from "train" subfolder of this dataset
    #???: ??? # will take all files from "train" subfolder of this dataset, relative to location of train.py script
  val:
    ShuffleMergeSpectral_trainingSamples-2_rerun_2TF: "../../../.." # will take all files from "val" subfolder of this dataset
    # ???: ??? # will take all files from "val" subfolder of this dataset, relative to location of train.py script

input_dataset_cfg: "../../../../${dataset_name}/cfg.yaml" # will fetch feature names and labels from here
# input_dataset_cfg: "???/${dataset_name}/cfg.yaml" # will fetch feature names and labels from here, relative to location of train.py script

# TF dataset formation
tf_dataset_cfg:
  combine_via: interleave # options: interleave, sampling
  n_threads: 10 # without controlling it may throw thread limit exception
  smart_batching_step: 10 # null to disable smart batching and use a standard one
  sequence_length_dist_start: 0 # for smart batching
  sequence_length_dist_end: 300 # for smart batching
  shuffle_buffer_size: 4000 # null to not shuffle
  shuffle_smart_buffer_size: 100
  cache: null # null to not cache the training dataset
  train_batch_size: 512
  val_batch_size: 512
  classes: ["tau", "e", "mu", "jet"] # will pick only those labels (in this order)

# LR schedule
schedule: null # custom // null // decrease
warmup_steps: null # only for schedule=custom, max LR = lr_multiplier * 1/sqrt(dim_model) * 1/sqrt(warmup_steps)
lr_multiplier: null # only for schedule=custom
learning_rate: 0.0001 # only for schedule=null
decrease_every: null # only for schedule=decrease
decrease_by: null # only for schedule=decrease

# optimiser
optimiser: adam # sgd // adam // adamw // radam
momentum: null # only for sgd
nesterov: null # only for sgd
weight_decay: null # only for adamw/radam
beta_1: 0.9 # only for adam/adamw/radam
beta_2: 0.999 # only for adam/adamw/radam
epsilon: 1e-7 # only for adam/adamw/radam

# training
n_epochs: 100
min_delta: 0.0001
patience: 3

# GPU settings
gpu:
  gpu_index: 0
  #gpu_index: [0,1,2,3,4,5,6,7]
  #gpu_index: [0,1,2,3]
  #gpu_index: [0,1]
  #gpu_mem: 38 # in Gb

# CPU settings
cpu:
  cores: 4
  intra: 4
  inter: 4