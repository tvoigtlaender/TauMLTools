hydra: # disable usage of hydra log directory as cwd
  job:
    chdir: False

defaults:
  - model@model: transformer
  - input_files: all_train_files
  - _self_

# mlflow
path_to_mlflow: ./mlruns
experiment_name: Jan_2025_Transformer

# datasets
dataset_name: january_2025_V1 # to log to mlflow for bookkeeping

# TF dataset formation
tf_dataset_cfg:
  datasets_location: 
    training: "data2/train/"
    validation: "data2/val/"
    dataset_cfg: "data2/cfg.yaml"
  combine_via: interleave # options: interleave, sampling
  n_threads: 10 # without controlling it may throw thread limit exception
  batching: smart # standard // smart // token
  # smart_batching_step: null # null to disable smart batching and use a standard one
  smart_batching_step: 10 # null to disable smart batching and use a standard one
  sequence_length_dist_start: 0 # for smart batching
  sequence_length_dist_end: 300 # for smart batching
  tokens_per_batch: 180000
  # train_tokens_per_batch: 180000
  # val_tokens_per_batch: 600000
  shuffle_buffer_size: 4000 # null to not shuffle
  shuffle_smart_buffer_size: 100
  cache: null # null to not cache the training dataset
  train_batch_size: 400
  val_batch_size: 800
  classes: ["tau", "e", "mu", "jet"] # will pick only those labels (in this order)
  scaler: True

# LR schedule
# schedule: dynamic # custom // null // decrease
schedule: null # custom // null // decrease
warmup_steps: null # only for schedule=custom, max LR = lr_multiplier * 1/sqrt(dim_model) * 1/sqrt(warmup_steps)
lr_multiplier: null # only for schedule=custom
# learning_rate: 0.0005 # only for schedule=null
learning_rate: 0.0001 # only for schedule=null
decrease_every: null # only for schedule=decrease
decrease_by: null # only for schedule=decrease

# optimiser
optimiser: adam # sgd // adam // adamw // radam
momentum: null # only for sgd
nesterov: null # only for sgd
weight_decay: null # only for adamw/radam
beta_1: 0.9 # only for adam/adamw/radam
beta_2: 0.999 # only for adam/adamw/radam
epsilon: 1e-7 # only for adam/adamw/radam

# training
n_epochs: 1
min_delta: 0.0001
patience: 10

# GPU settings
# gpu: "0"
gpu: "0" #,1" #,2,3" #,4,5,6,7"
#"0,1"
#,2,3,4,5,6,7"

# CPU settings
cpu:
  cores: 32
  intra: 16
  inter: 16
